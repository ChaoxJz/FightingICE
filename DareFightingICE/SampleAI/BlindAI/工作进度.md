# 10.15
本地部署DareFightingICE：
https://github.com/TeamFightingICE/FightingICE/tree/master/DareFightingICE/SampleAI/BlindAI	并执行BlindAI进行训练和直接对战

1.安装python3.8 并配置环境

2.安装miniconda py3.8 并配置环境
		系统变量path中加入如下
		1）***\Miniconda3； 
		2）***\Miniconda3\Scripts； 
		3）***\Miniconda3\Library\bin 

3.如git中顺序git bash
		(1)git clone https://github.com/TeamFightingICE/FightingICE.	 //clone
		(2)cd DareFightingICE/DareFightingICE/SampleAI/BlindAI	     //cd
		(3)conda env create -f environment.yml							             //复制同样环境
		(4)conda activate ice											                         //激活环境（可在vscode开始）

4.此时vscode中terminal已经可以使用conda指令
		遇到You may need to close and restart your shell after running 'conda init'.
			解决：
			https://blog.csdn.net/weixin_64064486/article/details/123972443?spm=1001.2101.3001.6650.2&utm_medium=distribute.pc_relevant.none-task-blog-2%7Edefault%7EBlogCommendFromBaidu%7ERate-2-123972443-blog-123829812.pc_relevant_3mothn_strategy_and_data_recovery&depth_1-utm_source=distribute.pc_relevant.none-task-blog-2%7Edefault%7EBlogCommendFromBaidu%7ERate-2-123972443-blog-123829812.pc_relevant_3mothn_strategy_and_data_recovery&utm_relevant_index=3

```
方法1：在VSCode terminal powershell 中执行conda init后重启vscode，忽略错误信息进行下一步。
方法2：Windows10 全局搜关键字powershell 以管理员权限打开,输入命令
set-ExecutionPolicy RemoteSigned

然后再输入‘Y’，之后回车确定执行命令就行。
再重启一下VSCode terminal powershell 输入命令 conda activate [虚拟环境名称]，就能发现终于正确切换conda 虚拟环境了
```

5.conda使用 python train.py -h
		报错UserWarning: No audio backend is available.
			conda中pip install PySoundFile
			安装完成后，输入进入python解释器，输入命令：import torchaudio，之后若出现代表python输入的符号：>>>，则证明torchaudio安装成功，即可用。

6.conda环境下执行python train.py -h 如下：
		usage: train.py [-h] [--encoder {conv1d,fft,mel}] [--port PORT] --id ID --p2 {Sandbox,MctsAi65,MctsAi} [--recurrent] [--n_frame N_FRAME]

		optional arguments:
		  -h, --help            show this help message and exit
		  --encoder {conv1d,fft,mel}
								Choose an encoder for the Blind AI
		  --port PORT           Port used by DareFightingICE
		  --id ID               Experiment id
		  --p2 {Sandbox,MctsAi65,MctsAi}
								The opponent AI
		  --recurrent           Use GRU
		  --n_frame N_FRAME     Number of frame to sample data

		  eg.python torch_train.py --p2 MctsAi65 --encoder fft --id rnn_1_frame_256_mctsai65 --n_frame 1 --recurrent
  
7.训练AI需要nonDelay，将BlindAI\non_delay下文件复制至FightingICE\for_nonDelay

8.将FightingICE_nonDelay.jar复制至FightingICE\

9.创建新bat文件，FightingICE_nonDelay.bat,编辑如下：
			::rem batファイルサンプル
			setlocal ENABLEDELAYEDEXPANSION
			::-Xmx8240m -Xms8240m -Xmn5120m 
			:: close CUDA code
			set CUDA_VISIBLE_DEVICES=100
			java -cp FightingICE_nonDelay.jar;./lib/lwjgl/*;./lib/natives/windows/*;./lib/*;  Main --py4j -n 1 --limithp 400 400 --grey-bg --inverted-player 1 --mute
			rem TIMEOUT /T -1
			endlocal
			pause
			exit
			同时解决之前打几个键就退出的问题，
			需要在FightingICE.bat的java加入--mute,但是FightingICE_nonDelay.bat则不受影响

10.vscode到目录下执行
		./FightingICE_nonDelay.bat

11.vscode进入conda ice环境到FightingICE-master\DareFightingICE\SampleAI\BlindAI下执行
		python train.py --p2 MctsAi65 --encoder fft --id rnn_1_frame_256_mctsai65 --n_frame 1 --recurrent



Issue（解决）：打完一局直接退出了 怀疑没设置循环 
		solution：顺着执行下来 又没有退出了

Issue：不nonDelay两边都不动
		改成了nondelay p2(MctsAi65)还是不动,MctsAi可以动
			改为python train.py --p2 MctsAi --encoder fft --id rnn_1_frame_256_mctsai65 --n_frame 1 --recurrentMctsAi
			MctsAi正常运作，怀疑是65没放好位置
		
Issue:一直打不停 不保存 NUM全改成1试试

```python
TODO_LIST 删除向上升龙拳，看不到的情况下用基本都空，最好调高冲击波和大冲击波的权重，听不到时这比较有用，以及蹲踢的权重，蹲题横向hitbox较长
```

```python
TODO_LIST（完成）：
	训练model时应该加入HP信息才对，这样model才会正向反馈（可能本来就已经加了？）
	实际上BlindAI的训练模型，在train时是可以知道双方血量，但拥有的信息就只是双方血量和声音信 
    号，除此以外没有任何信息（如双方位置）气槽的量未知
```

```
我谷那拿到的代码
public void getInformation(FrameData frameData, boolean arg1) { 		// TODO Auto-generated method stub 		this.frameData = frameData; 		this.commandCenter.setFrameData(this.frameData, playerNumber);  		myCharacter = frameData.getCharacter(playerNumber);
oppCharacter = frameData.getCharacter(!playerNumber);
```

# 10.16
#学习Transformer


RNN 还有一点不足，那就是训练慢。因为它默认是按时序来进行处理的，一个个单词词从左到右看过去，导致 RNN 不能像 CNN 一样，充分利用 GPU 的并行运算优势。

embedding 嵌入
	什么是深度学习里的Embedding？
	这个概念在深度学习领域最原初的切入点是所谓的Manifold Hypothesis（流形假设）。流形假设是指“自然的原始数据是低维的流形嵌入于(embedded in)原始数据所在的高维空间”。那么，深度学习的任务就是把高维原始数据（图像，句子）映射到低维流形，使得高维的原始数据被映射到低维流形之后变得可分，而这个映射就叫嵌入（Embedding）。比如WordEmbedding，就是把单词组成的句子映射到一个表征向量。但后来不知咋回事，开始把低维流形的表征向量叫做Embedding，其实是一种误用。。

# 10.22
1.装了jupyter notebook 和 jupyter lab 和 nb_conda https://www.cnblogs.com/catting123/p/16557462.html

2.在虚拟环境中安装jupyter https://blog.csdn.net/vincent_duan/article/details/116358392

```
1.eclipse版本更新导致java-se8以后需要删除monojavainfo

2.FightingICE.jar -mute导致运行不了是太老了，3年前的github，拿了前辈的就直接解决

3.MctsAi65不动，将新的src/aiinterface更新就可以动了具体看视频

4.更改transformer model，意思在encoder里增加一个新的encoder，从fft/mel变成transformer就可以了
```

```
步骤：
1)读论文，搞懂现在用的是什么东西在train，现在用了一个2层CNN，FFT和2层全连接网络FCN，和Mel结合CNN三种模式来将声音转化。所以引入transformer并不是更改ppo这个policy，而仅仅只是替换encoder

2)学习Transformer，做一个小的项目，语音识别&图像识别&翻译

3)尝试调整接口，接入transformer_encoder
```

```
学习的区别

监督学习：通过已有的训练样本（即已知数据以及其对应的输出）去训练得到一个最优模型（这个模型属于某个函数的集合，最优则表示在某个评价准则下是最佳的），再利用这个模型将所有的输入映射为相应的输出。

无监督学习：它与监督学习的不同之处，在于我们事先没有任何训练样本，而需要直接对数据进行建模。
半监督学习：在训练阶段结合了大量未标记的数据和少量标签数据。与使用所有标签数据的模型相比，使用训练集的训练模型在训练时可以更为准确。

强化学习：我们设定一个回报函数（reward function），通过这个函数来确认否越来越接近目标，类似我们训练宠物，如果做对了就给他奖励，做错了就给予惩罚，最后来达到我们的训练目的。
```




# 10.25
```python
TODO_LIST:
1.（完成，实例没做）学习并介绍一下transformer，最好训练一个实例，如画像识别（最好是声音识别，基于pytorch）

2.(没时间做)读论文并部署那个声音 https://github.com/hegde95/Agents_that_Listen

3.（800，2的输入，输出由输出层导致各有区别）问前辈能不能开zoom教教我，这个data怎么是一个random还是1x3200x2的三维矩阵 不应该是一个1000？（left+right=1000）
```

```
abc模组：
import abc  

abc（抽象基类）Python 对于ABC的支持模块，定义了一个特殊的metaclass—— ABCMeta 还有一些装饰器—— @abstractmethod 和 @abstarctproperty

https://zhuanlan.zhihu.com/p/508700685
```

```python
import abc  # 利用abc模块实现抽象类  
class File(metaclass=abc.ABCMeta):  # abc.ABCMeta是实现抽象类的一个基础类     
@abc.abstractmethod  # 定义抽象方法，无需实现功能     
def read(self):         
pass  
class Txt(File):  # 子类继承抽象类，但是必须定义read方法将抽象类中的read方法覆盖     
def read(self):         
print('文本数据的读取方法')  
txt1 = Txt() 
txt1.read() #可以正常打印
txt2 = File() 
txt2.read() #会报错 提示未实现
```

```Python
SOTA是英文：state of the art的缩写
在某项研究任务中：
1. SOTA model 是指在此类任务中，最好或者最先进的模型，
2. SOTA 结果，是指在此类任务中，目前最好模型的性能表现。
```


# 10.27
pytorch关联
		torch.cat(inputs,dim=0) 连接tensor张量
			dim=0 则连接第1维（2，3）接成（4，3）
			dim=1 则连接第2维（2，3）接成（2，6）
			dim等于n则连接第n维  	
				x1.shape:1x2x3x4
				x2.shape:1x2x3x4 
					dim=0 cat.shape=2x2x3x4
					dim=1 cat.shape=1x4x3x4
					dim=2 cat.shape=1x2x6x4
					dim=3 cat.shape=1x2x3x8
				x1.shape!=x2.shape 则必须其中n-1项都相等，而且cat的就是不相等那项才行
				
				.unsqueeze()
					增加维度，在tensor后面.unsqueeze(a) 
					像tensor的a维增加维度
					如x=([20])	x.unsqueeze(1) x.shape=(20,1)		x.unsqueeze(0) x.shape(1,20)


torch.exp(x):e^xi

torch.range(a,b,c)&torch.arange(a,b,c)
- a:start		b:end	c:gap
- range产生的长度是10-1+1=10 是由1到10组成的1维张量，类型float	size[10]
- arange产生的是10-1=9 由1-9组成的1维度张量 ，类型int				size[9]	


jupyter notebook
- enter进入编辑，esc退出
- a:向上创建cell	b：向下创建cell		dd：删除cell
- tab 自动补齐名称
- 函数后面 shift+tab 查看方法

transformer_model:
http://121.199.45.168:8001/1/

model_buffer&model_parameters:
https://zhuanlan.zhihu.com/p/578032290

parameter:一种是反向传播需要被optimizer更新的

buffer：一种是反向传播不需要被optimizer更新

softmax解释：
https://zhuanlan.zhihu.com/p/578032185

tf中multiply、matmul、dot、batch_dot区别：
https://zhuanlan.zhihu.com/p/578031931


# 10.28
torch.matmul
- 同维相乘
	- 相乘必须满足以下两个条件：
		1）两个n维数组的前n-2维必须完全相同。例如（3,2,4,2）（3,2,2,3）前两维必须完全一致；
		2）最后两维必须满足二阶矩阵乘法要求。例如（3,2,4,2）（3,2,2,3）的后两维可视为（4,2）x（2,3）满足矩阵乘法。
			这样的同维矩阵相乘，所得到的维度是：前n-2维不变，后2维进行矩阵乘法。
			例如，（3,2,4,2）*（1,2,2,3）——>>（3,2，4,3）
		torch中不可以不同维度matmul

	- 不同维度
		在使用numpy数组时，不同维度相乘无法使用matmul，而使用*进行的是逐元素相乘。
		会通过广播机制补充到同维度，再进行同维度的乘法。

torch.nn.linera():https://zhuanlan.zhihu.com/p/578339907

python-zip()
		a=[1,2,3]
		b=[5,6,7]
		c=zip(a,b)------>[(1,5),(2,6),(3,7)]组成一个新的元组
		
python语法 
		input:
			a,b,c=\
				[e,f,g]
		output:
			a=e,b=f,c=g
	
python语法中@classmethod等等 各种@ 搞不懂

```python
Todo list（已经解决）：找到声音raw audio data导入transformer模型应该设定什么size，输出什么size比较好的论文(得出结果后先压平，再通过全连接层变为512个输出)


Todo list（小桃在做）：输入input是声音,source是位置，output是位置。 课题变成了利用transformer模型从输入声音预测角色所在的位置。

Todo list：读懂游戏声音3种模型利用的论文，就知道需要转化成什么了。
```

10.29
环境中加入新的pip
	altair
	torchtext
	spacy
	GPUtil
	修改为torch_cpu为torch_cuda：conda install pytorch torchvision torchaudio pytorch-cuda=11.7 -c pytorch -c nvidia
	安装cuda

```Python
究极重要问题：源码中，EncoderLayer传入是（d_model）而不是size， EncoderLayer定义里的size似乎本来是想传src_vocab词表大小？还是写错了？
```

传入各项数据float-int-long_int等如何转化
https://openal-soft.org/openal-extensions/SOFT_loopback.txt
